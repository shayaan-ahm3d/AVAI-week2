from typing import NamedTuple
from enum import Enum
from pathlib import Path
import random

from PIL import Image
import torch
from torch import Tensor
from torchvision.transforms.functional import crop
from torch.utils.data import Dataset
from torchvision.transforms import ToTensor

class ImagePair(NamedTuple):
    low: Tensor
    high: Tensor


class Mode(Enum):
    TRAIN = 0
    VALIDATE = 1
    TEST = 2


NUM_CHANNELS = 3
class Div2kDataset(Dataset):
    def __init__(self, low_root: Path, high_root: Path, transform=None, mode: Mode=Mode.TRAIN) -> None:
        self.low_root = Path(low_root)
        self.high_root = Path(high_root)
        self.transform = transform
        self.mode = mode

        if self.transform is None:
            self.transform = ToTensor()

        self.low_paths = self._collect_files(self.low_root)
        self.high_paths = self._collect_files(self.high_root)

        if len(self.low_paths) != len(self.high_paths):
            raise ValueError(f"Mismatch: {len(self.low_paths)} LR vs {len(self.high_paths)} HR images.")

    def __len__(self) -> int:
        return len(self.low_paths)

    def __getitem__(self, idx: int) -> ImagePair:
        """Loads image upon access instead of loading entire dataset into memory"""
        low_image = Image.open(self.low_paths[idx])
        high_image = Image.open(self.high_paths[idx])

        low_image_tensor: Tensor = self.transform(low_image)
        high_image_tensor: Tensor = self.transform(high_image)

        pair = ImagePair(low_image_tensor, high_image_tensor)
        return pair

    @staticmethod
    def get_coordinate_to_pixel_value_mapping(image: Tensor) -> tuple[Tensor, Tensor]:
        """Used for implicit neural representations (INR) only"""
        coords: Tensor = get_mgrid(image.shape[1], image.shape[2])
        pixels: Tensor = image.permute(1, 2, 0).contiguous().view(image.shape[1] * image.shape[2], NUM_CHANNELS)

        return coords, pixels

    def _collect_files(self, root: Path) -> list[Path]:
        return sorted([
            p for p in Path(root).rglob("*")
            if p.is_file()
        ])

# Let's generate a grid of coordinates over a 2D space and reshape the output into a flattened format.
def get_mgrid(side_len1: int, side_len2: int, dim: int = 2) -> Tensor:
    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.
    sidelen: int
    dim: int'''

    if side_len1 >= side_len2:
      # use sidelen1 steps to generate the grid
      tensors = tuple(dim * [torch.linspace(-1, 1, steps = side_len1)])
      mgrid = torch.stack(torch.meshgrid(*tensors), dim = -1)
      # crop it along one axis to fit sidelen2
      minor = int((side_len1 - side_len2)/2)
      mgrid = mgrid[:,minor:side_len2 + minor]

    if side_len1 < side_len2:
      tensors = tuple(dim * [torch.linspace(-1, 1, steps = side_len2)])
      mgrid = torch.stack(torch.meshgrid(*tensors), dim = -1)

      minor = int((side_len2 - side_len1)/2)
      mgrid = mgrid[minor:side_len1 + minor,:]

    # flatten the gird
    mgrid = mgrid.reshape(-1, dim)

    return mgrid

class PatchedDataset(Dataset):
    def __init__(self, dataset, patch_size, scale):
        self.dataset = dataset
        self.patch_size = patch_size
        self.scale = scale

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        low, high = self.dataset[idx]
        # crop here to minimise memory usage
        low_patch, high_patch = get_random_patch(low, high, self.patch_size, self.scale)
        return low_patch, high_patch
    

def get_random_patch(low_res: Tensor, high_res: Tensor, patch_size: int, scale: int = 8) -> tuple[Tensor, Tensor]:
    _, low_res_height, low_res_width = low_res.shape
    
    # Calculate random crop position
    tx: int = random.randrange(0, low_res_width - patch_size + 1)
    ty: int = random.randrange(0, low_res_height - patch_size + 1)
    
    low_patch: Tensor = crop(low_res, ty, tx, patch_size, patch_size)
    
    # Crop HR (coordinates scaled)
    tx_hr, ty_hr = tx * scale, ty * scale
    high_patch_size: int = patch_size * scale
    high_patch: Tensor = crop(high_res, ty_hr, tx_hr, high_patch_size, high_patch_size)
    
    return low_patch, high_patch